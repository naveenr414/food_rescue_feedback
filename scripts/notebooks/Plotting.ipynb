{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "plt.style.use('default')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['axes.labelsize'] = 18\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "plt.rcParams['axes.prop_cycle'] = mpl.cycler(color=colors)\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "\n",
    "plt.rcParams['savefig.bbox'] = 'tight'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-Annotator Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "naveen_file = \"naveen_annotations.json\"\n",
    "jingwu_file = \"jingwu_annotations.json\"\n",
    "naveen_data = json.load(open(\"../../results/annotations/\"+naveen_file))\n",
    "jingwu_data = json.load(open(\"../../results/annotations/\"+jingwu_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "naveen_data['annotations'] = sorted(naveen_data['annotations'],key=lambda k: k['id'])\n",
    "jingwu_data['annotations'] = sorted(jingwu_data['annotations'],key=lambda k: k['id'])\n",
    "assert len(jingwu_data) == len(naveen_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inadequate_food 0.9320156755835747\n",
      "donor_problem 0.6956956956956957\n",
      "recipient_problem 0.7254335260115607\n",
      "info_update 0.5730337078651686\n"
     ]
    }
   ],
   "source": [
    "keys = ['inadequate_food', 'donor_problem', 'recipient_problem', 'info_update']\n",
    "for k in keys:\n",
    "    all_naveen_values = [i[k] for i in naveen_data['annotations']]\n",
    "    all_jingwu_values = [i[k] for i in jingwu_data['annotations']]\n",
    "    print(k,cohen_kappa_score(all_naveen_values, all_jingwu_values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_files = [\"gpt_35_annotations.json\",\"gpt_35_annotations_updated.json\"]#,\"gpt_4_annotations.json\"]\n",
    "consensus_file = \"consensus_annotations.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_data = json.load(open(\"../../results/annotations/\"+consensus_file))\n",
    "consensus_data['annotations'] = sorted(consensus_data['annotations'],key=lambda k: k['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_id = [i['id'] for i in consensus_data['annotations']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gpt-3.5 turbo\n",
      "0.9044585987261146 inadequate_food\n",
      "0.29032258064516125 donor_problem\n",
      "0.64 recipient_problem\n",
      "0.391304347826087 info_update\n",
      "\n",
      "Model gpt-3.5 turbo\n",
      "0.8875 inadequate_food\n",
      "0.2641509433962264 donor_problem\n",
      "0.368421052631579 recipient_problem\n",
      "0.41025641025641024 info_update\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for annotation_file in annotation_files:\n",
    "    annotations = json.load(open(\"../../results/annotations/\"+annotation_file))\n",
    "    annotations['annotations'] = sorted(annotations['annotations'],key=lambda k: k['id'])\n",
    "    assert [i['id'] for i in annotations['annotations']] == all_id\n",
    "    print(\"Model {}\".format(annotations['parameters']['model']))\n",
    "\n",
    "    keys = ['inadequate_food', 'donor_problem', 'recipient_problem', 'info_update']\n",
    "    for k in keys:\n",
    "        all_annotation_values = [i[k] for i in annotations['annotations']]\n",
    "        all_consensus_values = [i[k] for i in consensus_data['annotations']]\n",
    "\n",
    "        f1 = f1_score(all_consensus_values,all_annotation_values)\n",
    "        print(f1,k)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
