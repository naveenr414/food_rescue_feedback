{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, cohen_kappa_score\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "plt.style.use('default')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['xtick.labelsize'] = 18\n",
    "plt.rcParams['ytick.labelsize'] = 18\n",
    "plt.rcParams['axes.labelsize'] = 18\n",
    "plt.rcParams['legend.fontsize'] = 14\n",
    "plt.rcParams['axes.prop_cycle'] = mpl.cycler(color=colors)\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "\n",
    "plt.rcParams['savefig.bbox'] = 'tight'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-Annotator Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "naveen_file = \"naveen_annotations.json\"\n",
    "jingwu_file = \"jingwu_annotations.json\"\n",
    "naveen_data = json.load(open(\"../../results/annotations/\"+naveen_file))\n",
    "jingwu_data = json.load(open(\"../../results/annotations/\"+jingwu_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "naveen_data['annotations'] = sorted(naveen_data['annotations'],key=lambda k: k['id'])\n",
    "jingwu_data['annotations'] = sorted(jingwu_data['annotations'],key=lambda k: k['id'])\n",
    "assert len(jingwu_data) == len(naveen_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inadequate_food 0.9320156755835747\n",
      "donor_problem 0.6956956956956957\n",
      "recipient_problem 0.7254335260115607\n",
      "info_update 0.5730337078651686\n"
     ]
    }
   ],
   "source": [
    "keys = ['inadequate_food', 'donor_problem', 'recipient_problem', 'info_update']\n",
    "for k in keys:\n",
    "    all_naveen_values = [i[k] for i in naveen_data['annotations']]\n",
    "    all_jingwu_values = [i[k] for i in jingwu_data['annotations']]\n",
    "    print(k,cohen_kappa_score(all_naveen_values, all_jingwu_values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['recipient_problem', 'inadequate_food', 'donor_problem', \n",
    "            'direction_problem','earlier_pickup','system_problem',\n",
    "            'update_contact']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2024-05-20\"\n",
    "end_date = \"2024-05-26\"\n",
    "train_labels = pd.read_csv('../../results/annotations/naveen_hierarchical_annotations_{}_{}.csv'.format(start_date,end_date))\n",
    "train_predictions = pd.read_csv('../../results/reports/labeled_feedbacks_{}_{}.csv'.format(start_date,end_date))\n",
    "train_predictions[tasks] = train_predictions[tasks].astype(int)\n",
    "train_predictions['any_donor_problem'] = train_predictions[['inadequate_food', 'donor_problem','earlier_pickup']].max(axis=1)\n",
    "train_labels['any_donor_problem'] = train_labels[['inadequate_food', 'donor_problem','earlier_pickup']].max(axis=1)\n",
    "train_labels['any_problem'] = train_labels[tasks].max(axis=1)\n",
    "train_predictions['any_problem'] = train_predictions[tasks].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For task recipient_problem, precision 0.1111111111111111, recall 0.8, accuracy 0.7380952380952381\n",
      "For task inadequate_food, precision 0.85, recall 0.9444444444444444, accuracy 0.9682539682539683\n",
      "For task donor_problem, precision 0.25, recall 0.7777777777777778, accuracy 0.8174603174603174\n",
      "For task direction_problem, precision 0.2857142857142857, recall 1.0, accuracy 0.9206349206349206\n",
      "For task earlier_pickup, precision 0.75, recall 1.0, accuracy 0.9841269841269841\n",
      "For task system_problem, precision 1.0, recall 1.0, accuracy 1.0\n",
      "For task update_contact, precision 0.2222222222222222, recall 1.0, accuracy 0.9444444444444444\n",
      "For task any_donor_problem, precision 0.6829268292682927, recall 0.9032258064516129, accuracy 0.873015873015873\n",
      "For task any_problem, precision 0.7307692307692307, recall 0.926829268292683, accuracy 0.8650793650793651\n"
     ]
    }
   ],
   "source": [
    "for task in tasks + ['any_donor_problem','any_problem']:\n",
    "    preds = train_predictions[task]\n",
    "    labels = train_labels[task]\n",
    "    \n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    print(\"For task {}, precision {}, recall {}, accuracy {}\".format(task,precision,recall,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"2024-05-13\"\n",
    "end_date = \"2024-05-19\"\n",
    "validation_labels = pd.read_csv('../../results/annotations/naveen_hierarchical_annotations_{}_{}.csv'.format(start_date,end_date))\n",
    "validation_predictions = pd.read_csv('../../results/reports/labeled_feedbacks_{}_{}.csv'.format(start_date,end_date))\n",
    "validation_predictions[tasks] = validation_predictions[tasks].astype(int)\n",
    "validation_predictions['any_donor_problem'] = validation_predictions[['inadequate_food', 'donor_problem','earlier_pickup']].max(axis=1)\n",
    "validation_labels['any_donor_problem'] = validation_labels[['inadequate_food', 'donor_problem','earlier_pickup']].max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For task recipient_problem, precision 0.30434782608695654, recall 1.0, accuracy 0.873015873015873\n",
      "For task inadequate_food, precision 0.5925925925925926, recall 1.0, accuracy 0.9126984126984127\n",
      "For task donor_problem, precision 0.17647058823529413, recall 1.0, accuracy 0.8888888888888888\n",
      "For task direction_problem, precision 0.4, recall 1.0, accuracy 0.9523809523809523\n",
      "For task earlier_pickup, precision 0.2857142857142857, recall 1.0, accuracy 0.9603174603174603\n",
      "For task system_problem, precision 0.6, recall 1.0, accuracy 0.9841269841269841\n",
      "For task update_contact, precision 0.16666666666666666, recall 1.0, accuracy 0.9603174603174603\n",
      "For task any_donor_problem, precision 0.5135135135135135, recall 1.0, accuracy 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "for task in tasks + ['any_donor_problem']:\n",
    "    preds = validation_predictions[task]\n",
    "    labels = validation_labels[task]\n",
    "    \n",
    "    precision = precision_score(labels, preds)\n",
    "    recall = recall_score(labels, preds)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    print(\"For task {}, precision {}, recall {}, accuracy {}\".format(task,precision,recall,accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_files = [\"gpt_35_annotations.json\",\"gpt_35_annotations_updated.json\"]#,\"gpt_4_annotations.json\"]\n",
    "consensus_file = \"consensus_annotations.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_data = json.load(open(\"../../results/annotations/\"+consensus_file))\n",
    "consensus_data['annotations'] = sorted(consensus_data['annotations'],key=lambda k: k['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_id = [i['id'] for i in consensus_data['annotations']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model gpt-3.5 turbo\n",
      "0.9044585987261146 inadequate_food\n",
      "0.29032258064516125 donor_problem\n",
      "0.64 recipient_problem\n",
      "0.391304347826087 info_update\n",
      "\n",
      "Model gpt-3.5 turbo\n",
      "0.8875 inadequate_food\n",
      "0.2641509433962264 donor_problem\n",
      "0.368421052631579 recipient_problem\n",
      "0.41025641025641024 info_update\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for annotation_file in annotation_files:\n",
    "    annotations = json.load(open(\"../../results/annotations/\"+annotation_file))\n",
    "    annotations['annotations'] = sorted(annotations['annotations'],key=lambda k: k['id'])\n",
    "    assert [i['id'] for i in annotations['annotations']] == all_id\n",
    "    print(\"Model {}\".format(annotations['parameters']['model']))\n",
    "\n",
    "    keys = ['inadequate_food', 'donor_problem', 'recipient_problem', 'info_update']\n",
    "    for k in keys:\n",
    "        all_annotation_values = [i[k] for i in annotations['annotations']]\n",
    "        all_consensus_values = [i[k] for i in consensus_data['annotations']]\n",
    "\n",
    "        f1 = f1_score(all_consensus_values,all_annotation_values)\n",
    "        print(f1,k)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trip Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_json = json.load(open(\"../../results/annotations/scored_instructions.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'logistics': 3, 'contact': 7, 'unhelpful': 1, 'directions': 4})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_count = Counter([i['category'] for i in annotated_json])\n",
    "category_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'side': 'donor',\n",
       " 'location_id': 123,\n",
       " 'comment': 'Giant eagle says only staurday Sunday and monday for 412 food rescue.   They said remove them from other days.  \\r\\n',\n",
       " 'old_instruction': \"Please enter the 412 Food Rescue PIN into scanner: 65428. Please do not share the PIN # with staff--it is our electronic signature! . Please call store when you're on your way to confirm donation. Ask for Bakery to confirm a donation. \",\n",
       " 'new_instruction': \"Please enter the 412 Food Rescue PIN into scanner: 65428. Please do not share the PIN # with staff--it is our electronic signature! . Please call store when you're on your way to confirm donation. Ask for Bakery to confirm a donation. Only available for pickup on Saturday, Sunday, and Monday.\",\n",
       " 'informativeness': 1,\n",
       " 'clarity': 1,\n",
       " 'helpfulness': 1,\n",
       " 'truthfulness': 1,\n",
       " 'category': 'logistics'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotated_json[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "informativeness 1.0\n",
      "clarity 0.9333333333333333\n",
      "helpfulness 0.9333333333333333\n",
      "truthfulness 1.0\n"
     ]
    }
   ],
   "source": [
    "metrics = ['informativeness','clarity','helpfulness','truthfulness']\n",
    "for m in metrics:\n",
    "    print(m,Counter([i[m] for i in annotated_json])[1]/len(annotated_json))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "food",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
